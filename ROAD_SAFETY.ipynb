{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3hj2zIASO/itrBN0olsu2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyadharshan-SC/Catalyst-Crew/blob/main/ROAD_SAFETY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# 'xgboost' is not available, so we use its scikit-learn equivalent\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def train_on_new_dataset():\n",
        "    \"\"\"\n",
        "    Loads, preprocesses, and trains a Gradient Boosting model on the\n",
        "    'Road.csv' dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    file_name = \"/content/Road.csv\" # The file from the Kaggle link\n",
        "\n",
        "    # --- 1. Load Data ---\n",
        "    try:\n",
        "        df = pd.read_csv(file_name)\n",
        "        print(f\"Successfully loaded '{file_name}'.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"--- ERROR ---\")\n",
        "        print(f\"File not found: '{file_name}'\")\n",
        "        print(\"Please upload the 'Road.csv' file from the Kaggle link first.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 2. Preprocessing ---\n",
        "\n",
        "    # Define Target (y) and Features (X)\n",
        "    target_column = 'Accident_severity' # The target in this new dataset\n",
        "\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"Error: Target column '{target_column}' not found in the file.\")\n",
        "        return\n",
        "\n",
        "    # Encode the target variable (e.g., \"Slight Injury\", \"Serious Injury\")\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(df[target_column])\n",
        "\n",
        "    X = df.drop(columns=[target_column])\n",
        "\n",
        "    # Drop columns that are text-based or hard to use directly\n",
        "    # 'Time' is a string 'HH:MM:SS' that needs feature engineering,\n",
        "    # so we drop it for this baseline model.\n",
        "    cols_to_drop = ['Time']\n",
        "\n",
        "    # Find which of these columns actually exist in the DataFrame to avoid errors\n",
        "    existing_cols_to_drop = [col for col in cols_to_drop if col in X.columns]\n",
        "    X = X.drop(columns=existing_cols_to_drop)\n",
        "\n",
        "    # Automatically find numeric and categorical feature names\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "    print(f\"Found {len(numeric_features)} numeric features.\")\n",
        "    print(f\"Found {len(categorical_features)} categorical features.\")\n",
        "\n",
        "    # Create preprocessing pipelines for both data types\n",
        "\n",
        "    # Numeric transformer: fills missing values with the median\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median'))\n",
        "    ])\n",
        "\n",
        "    # Categorical transformer: fills missing values and then one-hot encodes\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numeric and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough' # Keep any columns we didn't specify\n",
        "    )\n",
        "\n",
        "    # --- 3. Define the Gradient Boosting Model ---\n",
        "\n",
        "    # We use GradientBoostingClassifier as the alternative to XGBoost\n",
        "    model = GradientBoostingClassifier(\n",
        "        n_estimators=150,       # Number of trees. Start here.\n",
        "        max_depth=5,            # Max depth of each tree.\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        max_features=0.8,       # Equivalent to 'colsample_bytree'\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # --- 4. Create and Train the Full Pipeline ---\n",
        "\n",
        "    clf_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining the model on 'Road.csv'...\")\n",
        "    clf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # --- 5. Evaluate the Model ---\n",
        "    y_pred = clf_pipeline.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\n--- Model Evaluation ---\")\n",
        "    print(f\"Target Accuracy Range: 82.00% - 88.00%\")\n",
        "    print(f\"Final Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    if 0.82 <= accuracy <= 0.88:\n",
        "        print(\"✅ Success! Accuracy is within the target range.\")\n",
        "    else:\n",
        "        print(\"\\nThis is your baseline accuracy. Now you can tune it!\")\n",
        "        print(\"To improve accuracy, try increasing 'n_estimators' (e.g., to 250) or 'max_depth' (e.g., to 7).\")\n",
        "        print(\"If accuracy is too high (overfitting), try decreasing them.\")\n",
        "\n",
        "# --- Run the script ---\n",
        "if __name__ == \"__main__\":\n",
        "    train_on_new_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1YWuFmyRxWb",
        "outputId": "3ff5d922-ce66-47b6-b753-3820ca13c6c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded '/content/Road.csv'.\n",
            "Found 2 numeric features.\n",
            "Found 28 categorical features.\n",
            "\n",
            "Training the model on 'Road.csv'...\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Target Accuracy Range: 82.00% - 88.00%\n",
            "Final Model Accuracy: 84.54%\n",
            "✅ Success! Accuracy is within the target range.\n"
          ]
        }
      ]
    }
  ]
}